# Lab 4.3 - Using the VectorDB for semantic search with KernelMemory

In the previous labs, we learned how to work with documents and basic RAG patterns. Now we're taking a significant step forward by implementing a vector database solution using `KernelMemory`. 

A vector database stores documents as high-dimensional mathematical vectors (embeddings) that capture the semantic meaning of the text. This enables **semantic search** - finding relevant information based on meaning and context rather than exact keyword matches. For example, searching for "small bag" could return results about "mini backpack" or "compact purse" because they share similar semantic meaning, even though the exact words don't match.

In this lab, you'll work with the vector database from the previous lab, that has been populated with PDF documents. With this vector database foundation, you can retrieve the right data from large document sets and leverage the LLM to format and present it intelligently. Even more powerful, you can perform diverse searches and queries about the data stored in your database, opening up new possibilities for knowledge discovery and information retrieval.

### Steps

#### 1. Open the solution 

In the **Solution Explorer** in your codespace by right-clicking the solution file and choose `Open Solution`

#### 2. Add a new method called RAG_with_memory

```csharp
public async Task RAG_with_memory(IConfiguration config)
{
    var memoryConnector = GetLocalKernelMemory(config);

    var question =
        """
        I booked tickets for a concert tonight in venue AFAS Live!.
        I have this small black backpack, not big like for school, more like the mini
        festival type ðŸ˜…. it just fits my wallet, a hoodie and a bottle of water.
        Is this allowed? 
        """;

    var response = await memoryConnector.AskAsync(question);
    Console.WriteLine("******** RESPONSE WITH MEMORY ***********");
    Console.WriteLine(response.Result);
}
```

What we see in above code is the following:

- Initialize Memory Connection: Creates a connection to your local vector database that we created in the previous lab.
- Define the Question: Sets up a natural language question about concert venue bag policies using descriptive terms
- Execute Semantic Search: Calls `AskAsync()` to search the vector database for relevant document chunks and generate an answer using the LLM based on found information
- Display Results: Shows the AI-generated answer to your question and as extra context.

#### 3. Call the RAG_with_memory method from Program.cs

In the `Program.cs` file, call the `RAG_with_memory` method. You can comment out the call to `IngestDocuments` from the previous lab because we have already built our database.

```csharp
await new ChatWithRag().RAG_with_memory(config);
```
#### 4. Run the application

Run the application and observe the output. You should see a response generated by the LLM based on the documents stored in the vector database.

### 5. Now try with other searching questions

Try changing the question to see how the system retrieves different information based on semantic meaning. Here are some example questions you can try:

```text
- "What items are prohibited at the Ziggo Dome venue?"
- "Can I bring a camera to the concert at AFAS Live?"
- "What is the bag policy for events at the Johan Cruijff ArenA?"
```

Implement an `AskVenueQuestion` method to ask about venue policies.

```csharp
public async Task AskVenueQuestion(IConfiguration config)
{
    var memoryConnector = GetLocalKernelMemory(config);
    var question =
        """
        Which venue allows a backpack?
        """;
    var response = await memoryConnector.AskAsync(question);
    Console.WriteLine("******** RESPONSE WITH MEMORY ***********");
    Console.WriteLine(response.Result);
}
```

### The End

This concludes lab 4.3. You have learned how to build an advanced RAG system using `KernelMemory` and a vector database to perform semantic search and generate context-aware responses based on ingested documents.
