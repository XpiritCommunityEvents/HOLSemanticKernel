# Lab 4.3 - Using the VectorDB for semantic search with KernelMemory

In the previous labs, we learned how to work with documents and basic RAG patterns. Now we're taking a significant step forward by implementing a vector database solution using KernelMemory. 

A vector database stores documents as high-dimensional mathematical vectors (embeddings) that capture the semantic meaning of the text. This enables **semantic search** - finding relevant information based on meaning and context rather than exact keyword matches. For example, searching for "small bag" could return results about "mini backpack" or "compact purse" because they share similar semantic meaning, even though the exact words don't match.

In this lab, you'll work with the vector database from the previous lab, that has been populated with PDF documents. With this vector database foundation, you can retrieve the right data from large document sets and leverage the LLM to format and present it intelligently. Even more powerful, you can perform diverse searches and queries about the data stored in your database, opening up new possibilities for knowledge discovery and information retrieval.

### Steps

#### 1. Open the solution 
In the solution explorer in your codespace by right-clickin the solution file and choose `Open Solution`

#### 2. Add a new function called Rag_With_Memory

```csharp
public async Task RaG_With_Memory(string deploymentName, string endpoint, string apiKey, IConfiguration config)
{
    var memoryConnector = GetLocalKernelMemory(deploymentName, endpoint, apiKey);

    var question =
        """
        I booked tickets for a concert tonight in venue AFAS Live!.
        I have this small black backpack, not big like for school, more like the mini
        festival type ðŸ˜…. it just fits my wallet, a hoodie and a bottle of water.
        Is this allowed? 
        """;

    var response = memoryConnector.AskAsync(question);
    Console.WriteLine("******** RESPONSE WITH MEMORY ***********");
    Console.WriteLine(response.Result.Result);

}
```

What we see in above code is the following:
- Initialize Memory Connection: Creates a connection to your local vector database that we created in the previous lab.
- Define the Question: Sets up a natural language question about concert venue bag policies using descriptive terms
- Execute Semantic Search: Calls AskAsync() to search the vector database for relevant document chunks and generate an answer using the LLM based on found information
- Display Results: Shows the AI-generated answer to your question and as extra context.

#### 3. Call the Rag_With_Memory method from Main
In the `Program.cs` file, call the `RaG_With_Memory` method from the `Main` method.

```csharp
await new ChatWithRag().RaG_With_Memory(model, endpoint, token, config);
```
#### 4. Run the application
Run the application and observe the output. You should see a response generated by the LLM based on the documents stored in the vector database.

### 5. Now try with other searching questions
Try changing the question to see how the system retrieves different information based on semantic meaning. Here are some example questions you can try:

```text
- "What items are prohibited at the Ziggo Dome venue?"
- "Can I bring a camera to the concert at AFAS Live?"
- "What is the bag policy for events at the Johan Cruijff ArenA?"
```

Implement a AskVenueQuestion method to ask about venue policies.

```csharp
internal async Task AskVenueQuestion(string deploymentName, string endpoint, string apiKey, IConfiguration config)
{
    var memoryConnector = GetLocalKernelMemory(deploymentName, endpoint, apiKey);
    var question =
        """
        Which venue allows a backpack?
        """;
    var response = memoryConnector.AskAsync(question);
    Console.WriteLine("******** RESPONSE WITH MEMORY ***********");
    Console.WriteLine(response.Result.Result);

}
```

### The End
This concludes lab 4.3. You have learned how to build an advanced RAG system using KernelMemory and a vector database to perform semantic search and generate context-aware responses based on ingested documents.
